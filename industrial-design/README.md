# Industrial Design Skill

A Claude skill that transforms product concepts into manufacturable design specifications through a structured 6-phase workflow.

## Origin

Created from `industrial-design-agent.md` â€” a 520-line system prompt originally written for Claude Code / CLI harness use. Converted to a modular skill with progressive disclosure (SKILL.md + 7 reference files) so it only loads what's needed per phase.

## Skill Structure

```
industrial-design/
â”œâ”€â”€ SKILL.md                        # Core instructions (174 lines)
â”‚                                     Identity, principles, 6-phase workflow, reference table
â”œâ”€â”€ references/
â”‚   â”œâ”€â”€ capability-check.md         # Tool detection + fallback matrix
â”‚   â”œâ”€â”€ research-workstreams.md     # 4 research workstreams + image management + IP disclaimer
â”‚   â”œâ”€â”€ rendering-pipeline.md       # L1/L2/L3 fidelity levels + DTS verification loop
â”‚   â”œâ”€â”€ artifact-registry.md        # Naming conventions, required artifacts, YAML source-of-truth files
â”‚   â”œâ”€â”€ engineering-standards.md    # Spec integrity policy, units, tolerancing, GD&T, rounding
â”‚   â”œâ”€â”€ costing-policy.md           # Anti-hallucination rules for cost/material claims
â”‚   â””â”€â”€ spec-template.md            # Brief intake template + Phase 6 final spec format
â”œâ”€â”€ evals/
â”‚   â””â”€â”€ evals.json                  # 3 test cases with assertions
â”œâ”€â”€ scripts/                        # (placeholder â€” no scripts needed yet)
â””â”€â”€ assets/                         # (placeholder â€” no assets needed yet)
```

## How It Works

The skill guides Claude through 6 phases with explicit gates:

1. **Intake** (ðŸ”“) â€” Parse brief against template, capability check, clarifying questions
2. **Research** (ðŸ”“) â€” Competitive analysis, materials, standards, visual references
3. **Ideation** (ðŸ”’) â€” 2-3 concepts with L1 sketches. Hard stop for user selection.
4. **Refinement** (ðŸ”’) â€” Mood boards, L2 renders, dimensioned sketches. Hard stop for approval.
5. **FMEA** (ðŸ”“) â€” Failure mode analysis, mitigations folded into spec
6. **Final Spec** (ðŸ”’) â€” Technical drawings, hero render, full spec sheet. Hard stop for sign-off.

Reference files load only when the workflow reaches their phase, keeping context lean.

## Installation

Copy `industrial-design/` into your skills directory:
- Cowork: `.skills/skills/industrial-design/`
- Claude Code: `.claude/skills/industrial-design/`

The skill triggers on keywords like "product design", "industrial design", "DFM", "design spec", "material selection", or casual phrases like "design me a tool" or "I have a product idea."

---

## Eval Framework

We built a lightweight eval harness to test the skill before shipping. This section documents how it works so you can reuse the pattern for other skills.

### What We Tested

Three eval prompts that exercise different Phase 1 behaviors:

| Eval | Prompt | Tests |
|------|--------|-------|
| 1 | Magnetic torpedo level (65% complete brief) | Capability check, clarifying questions, missing field identification |
| 2 | Premium hand trowel (80% complete brief) | Targeted (not broad) questions, DFM reasoning, artifact naming, spec labeling |
| 3 | Modular scaffolding connector (30% complete brief) | Vague brief handling, safety emphasis, standards identification, no fabricated specs |

### Eval Structure

```
industrial-design-workspace/
â”œâ”€â”€ eval-1/
â”‚   â””â”€â”€ with_skill/
â”‚       â”œâ”€â”€ inputs/              # (empty for these evals â€” no input files needed)
â”‚       â”œâ”€â”€ outputs/
â”‚       â”‚   â””â”€â”€ transcript.md    # Full execution transcript
â”‚       â””â”€â”€ grading.json         # Pass/fail per assertion with evidence
â”œâ”€â”€ eval-2/
â”‚   â””â”€â”€ with_skill/
â”‚       â”œâ”€â”€ outputs/
â”‚       â”‚   â””â”€â”€ transcript.md
â”‚       â””â”€â”€ grading.json
â””â”€â”€ eval-3/
    â””â”€â”€ with_skill/
        â”œâ”€â”€ outputs/
        â”‚   â””â”€â”€ transcript.md
        â””â”€â”€ grading.json
```

### How to Run Evals

**Step 1: Execute.** For each eval in `evals/evals.json`, spawn an agent (or run inline) that:
- Reads SKILL.md and relevant reference files
- Executes the eval prompt following the skill's instructions
- Saves a transcript to `workspace/eval-N/with_skill/outputs/transcript.md`

**Step 2: Grade.** For each transcript, evaluate against the assertions in `evals.json`:
- Search the transcript for evidence of each assertion
- PASS = clear evidence the assertion is true
- FAIL = no evidence, or evidence contradicts the assertion
- Save results to `workspace/eval-N/with_skill/grading.json`

**Step 3: Review.** Check pass rates and the grader's `eval_feedback` field for suggestions on tightening assertions.

### Reviewing Results

Each `grading.json` contains per-assertion pass/fail verdicts with cited evidence, plus two useful fields:

- **`claims`** â€” Facts the agent stated during execution, verified against the transcript. Catches hallucinated specs even when all assertions pass.
- **`eval_feedback`** â€” The grader's suggestions for tightening weak assertions or adding missing coverage. Use this to iterate on `evals.json` between runs.

### Assertion Design Notes

Good assertions for skills like this are **discriminating** â€” they pass when the skill genuinely works and fail when it doesn't. Some patterns that worked:

- **Negative assertions** ("does NOT fabricate specs", "does NOT skip to sketches") catch the most common failure mode: the agent racing ahead without following the workflow.
- **Domain-specific assertions** ("emphasizes safety for scaffolding") verify the agent adapts its response to the product category rather than giving generic output.
- **Completeness checks** ("identifies missing fields", "asks at least 2 questions") verify the agent follows the brief template rather than winging it.
- **Labeling assertions** ("cost claims labeled as Verified/Proposed/User Requirement") catch spec hallucination â€” the highest-risk failure mode for hardware design skills.