# Industrial Design Skill

A Claude skill that transforms product concepts into manufacturable design specifications through a structured 6-phase workflow.

## Origin

Created from `industrial-design-agent.md` â€” a 520-line system prompt originally written for Claude Code / CLI harness use. Converted to a modular skill with progressive disclosure (SKILL.md + 9 reference files) so it only loads what's needed per phase.

## Skill Structure

```
industrial-design/
â”œâ”€â”€ SKILL.md                        # Core instructions â€” identity, principles, 6-phase workflow
â”œâ”€â”€ industrial-design.skill         # Packaged skill (zip) for Claude Desktop upload
â”œâ”€â”€ references/
â”‚   â”œâ”€â”€ capability-check.md         # Tool detection + fallback matrix
â”‚   â”œâ”€â”€ claude-md-template.md       # CLAUDE.md project context template (cross-session memory)
â”‚   â”œâ”€â”€ design-system.md            # Visual design system â€” colors, type, components, CSS boilerplate
â”‚   â”œâ”€â”€ research-workstreams.md     # 4 research workstreams + orchestrator delegation + image strategy
â”‚   â”œâ”€â”€ research-subagent-prompt.md # Subagent prompt template with {{VARIABLE}} placeholders
â”‚   â”œâ”€â”€ rendering-pipeline.md       # L1/L2/L3 fidelity levels + DTS verification loop
â”‚   â”œâ”€â”€ artifact-registry.md        # Naming conventions, required artifacts, YAML source-of-truth files
â”‚   â”œâ”€â”€ engineering-standards.md    # Spec integrity policy, units, tolerancing, GD&T, rounding
â”‚   â”œâ”€â”€ costing-policy.md           # Anti-hallucination rules for cost/material claims
â”‚   â””â”€â”€ spec-template.md            # Brief intake template + Phase 6 final spec format
â”œâ”€â”€ evals/
â”‚   â””â”€â”€ evals.json                  # 3 test cases with assertions
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ init-artifact.sh            # Scaffold React + Tailwind + shadcn/ui artifact project
â”‚   â”œâ”€â”€ bundle-artifact.sh          # Bundle artifact into single-file HTML
â”‚   â””â”€â”€ shadcn-components.tar.gz    # Pre-packaged shadcn/ui components
â””â”€â”€ assets/
```

## How It Works

The skill guides Claude through 6 phases with explicit gates:

1. **Intake** (ðŸ”“) â€” Parse brief against template, capability check, clarifying questions, project scaffolding (creates `CLAUDE.md` + directory structure)
2. **Research** (ðŸ”“) â€” Lead agent acts as **orchestrator**: reads `research-subagent-prompt.md` template, fills variables per workstream, spawns **4 parallel subagents** (Task tool, `general-purpose` type, `haiku` model) for competitive intel, materials, standards, and visual references. Synthesizes and cross-references after all complete.
3. **Ideation** (ðŸ”’) â€” 2-3 concepts with L1 sketches. Hard stop for user selection.
4. **Refinement** (ðŸ”’) â€” Design language brief, mood boards, L2 renders, dimensioned sketches. Optional canvas-design skill for high-fidelity mood boards. Hard stop for approval.
5. **FMEA** (ðŸ”“) â€” Failure mode analysis, mitigations folded into spec
6. **Final Spec** (ðŸ”’) â€” Technical drawings, hero render, full spec sheet. Hard stop for sign-off.

`CLAUDE.md` is updated at every phase gate so new sessions can resume mid-project. Reference files load only when the workflow reaches their phase, keeping context lean.

### Design System

All HTML artifacts share a consistent visual language (Modern Product Studio style) defined in `references/design-system.md` â€” warm earth tones, system font stacks, standardized components (cards, tables, tags, annotations). No external stylesheets or fonts (blocked by Cowork CSP).

### Image Strategy

Visual reference boards use **base64 data URI embedding** for images. A two-stage build process handles this: the research subagent collects external image URLs, then the lead agent fetches and base64-encodes them into the final HTML. This ensures images render in all viewers including Cowork's CSP-restricted artifact viewer.

## Installation

Copy `industrial-design/` into your skills directory:
- Cowork: `.skills/skills/industrial-design/`
- Claude Code: `.claude/skills/industrial-design/`

The skill triggers on keywords like "product design", "industrial design", "DFM", "design spec", "material selection", or casual phrases like "design me a tool" or "I have a product idea."

---

## Eval Framework

We built a lightweight eval harness to test the skill before shipping. This section documents how it works so you can reuse the pattern for other skills.

### What We Tested

Three eval prompts that exercise different Phase 1 behaviors:

| Eval | Prompt | Tests |
|------|--------|-------|
| 1 | Magnetic torpedo level (65% complete brief) | Capability check, clarifying questions, missing field identification |
| 2 | Premium hand trowel (80% complete brief) | Targeted (not broad) questions, DFM reasoning, artifact naming, spec labeling |
| 3 | Modular scaffolding connector (30% complete brief) | Vague brief handling, safety emphasis, standards identification, no fabricated specs |

### Eval Structure

```
industrial-design-workspace/
â”œâ”€â”€ eval-1/
â”‚   â””â”€â”€ with_skill/
â”‚       â”œâ”€â”€ inputs/              # (empty for these evals â€” no input files needed)
â”‚       â”œâ”€â”€ outputs/
â”‚       â”‚   â””â”€â”€ transcript.md    # Full execution transcript
â”‚       â””â”€â”€ grading.json         # Pass/fail per assertion with evidence
â”œâ”€â”€ eval-2/
â”‚   â””â”€â”€ with_skill/
â”‚       â”œâ”€â”€ outputs/
â”‚       â”‚   â””â”€â”€ transcript.md
â”‚       â””â”€â”€ grading.json
â””â”€â”€ eval-3/
    â””â”€â”€ with_skill/
        â”œâ”€â”€ outputs/
        â”‚   â””â”€â”€ transcript.md
        â””â”€â”€ grading.json
```

### How to Run Evals

**Step 1: Execute.** For each eval in `evals/evals.json`, spawn an agent (or run inline) that:
- Reads SKILL.md and relevant reference files
- Executes the eval prompt following the skill's instructions
- Saves a transcript to `workspace/eval-N/with_skill/outputs/transcript.md`

**Step 2: Grade.** For each transcript, evaluate against the assertions in `evals.json`:
- Search the transcript for evidence of each assertion
- PASS = clear evidence the assertion is true
- FAIL = no evidence, or evidence contradicts the assertion
- Save results to `workspace/eval-N/with_skill/grading.json`

**Step 3: Review.** Check pass rates and the grader's `eval_feedback` field for suggestions on tightening assertions.

### Reviewing Results

Each `grading.json` contains per-assertion pass/fail verdicts with cited evidence, plus two useful fields:

- **`claims`** â€” Facts the agent stated during execution, verified against the transcript. Catches hallucinated specs even when all assertions pass.
- **`eval_feedback`** â€” The grader's suggestions for tightening weak assertions or adding missing coverage. Use this to iterate on `evals.json` between runs.

### Assertion Design Notes

Good assertions for skills like this are **discriminating** â€” they pass when the skill genuinely works and fail when it doesn't. Some patterns that worked:

- **Negative assertions** ("does NOT fabricate specs", "does NOT skip to sketches") catch the most common failure mode: the agent racing ahead without following the workflow.
- **Domain-specific assertions** ("emphasizes safety for scaffolding") verify the agent adapts its response to the product category rather than giving generic output.
- **Completeness checks** ("identifies missing fields", "asks at least 2 questions") verify the agent follows the brief template rather than winging it.
- **Labeling assertions** ("cost claims labeled as Verified/Proposed/User Requirement") catch spec hallucination â€” the highest-risk failure mode for hardware design skills.